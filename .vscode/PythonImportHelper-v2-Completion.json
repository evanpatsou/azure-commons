[
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "DataFrame",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "SparkSession",
        "importPath": "pyspark.sql",
        "description": "pyspark.sql",
        "isExtraImport": true,
        "detail": "pyspark.sql",
        "documentation": {}
    },
    {
        "label": "col",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "max",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "abs",
        "importPath": "pyspark.sql.functions",
        "description": "pyspark.sql.functions",
        "isExtraImport": true,
        "detail": "pyspark.sql.functions",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "patch",
        "importPath": "unittest.mock",
        "description": "unittest.mock",
        "isExtraImport": true,
        "detail": "unittest.mock",
        "documentation": {}
    },
    {
        "label": "AzureCredentials",
        "importPath": "azure_credentials",
        "description": "azure_credentials",
        "isExtraImport": true,
        "detail": "azure_credentials",
        "documentation": {}
    },
    {
        "label": "ClientSecretCredential",
        "importPath": "azure.identity",
        "description": "azure.identity",
        "isExtraImport": true,
        "detail": "azure.identity",
        "documentation": {}
    },
    {
        "label": "ClientSecretCredential",
        "importPath": "azure.identity",
        "description": "azure.identity",
        "isExtraImport": true,
        "detail": "azure.identity",
        "documentation": {}
    },
    {
        "label": "ClientSecretCredential",
        "importPath": "azure.identity",
        "description": "azure.identity",
        "isExtraImport": true,
        "detail": "azure.identity",
        "documentation": {}
    },
    {
        "label": "BlobServiceClient",
        "importPath": "azure.storage.blob",
        "description": "azure.storage.blob",
        "isExtraImport": true,
        "detail": "azure.storage.blob",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "AzureCredentialFactory",
        "importPath": "azure_data_loader.credentials.credential_factory",
        "description": "azure_data_loader.credentials.credential_factory",
        "isExtraImport": true,
        "detail": "azure_data_loader.credentials.credential_factory",
        "documentation": {}
    },
    {
        "label": "AzureDataLoader",
        "importPath": "azure_data_loader.azure_data_loader",
        "description": "azure_data_loader.azure_data_loader",
        "isExtraImport": true,
        "detail": "azure_data_loader.azure_data_loader",
        "documentation": {}
    },
    {
        "label": "CSVDataLoader",
        "importPath": "azure_data_loader.data_loader.csv_data_loader",
        "description": "azure_data_loader.data_loader.csv_data_loader",
        "isExtraImport": true,
        "detail": "azure_data_loader.data_loader.csv_data_loader",
        "documentation": {}
    },
    {
        "label": "ChunkedDataProcessor",
        "importPath": "azure_data_loader.data_processor.chunked_data_processor",
        "description": "azure_data_loader.data_processor.chunked_data_processor",
        "isExtraImport": true,
        "detail": "azure_data_loader.data_processor.chunked_data_processor",
        "documentation": {}
    },
    {
        "label": "SparkInitializationError",
        "kind": 6,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "class SparkInitializationError(Exception):\n    pass\nclass DataLoadError(Exception):\n    pass\nclass DataSaveError(Exception):\n    pass\nclass ConfigurationError(Exception):\n    pass\nclass AzureTableOperations:\n    \"\"\"Handles reading from and saving to abfss, reading and saving to catalog, joining tables, running SQL queries, ",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "DataLoadError",
        "kind": 6,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "class DataLoadError(Exception):\n    pass\nclass DataSaveError(Exception):\n    pass\nclass ConfigurationError(Exception):\n    pass\nclass AzureTableOperations:\n    \"\"\"Handles reading from and saving to abfss, reading and saving to catalog, joining tables, running SQL queries, \n    and performing date-related operations using PySpark.\"\"\"\n    def __init__(self, enable_hive_support: bool = False, initial_spark_config: dict = None):",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "DataSaveError",
        "kind": 6,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "class DataSaveError(Exception):\n    pass\nclass ConfigurationError(Exception):\n    pass\nclass AzureTableOperations:\n    \"\"\"Handles reading from and saving to abfss, reading and saving to catalog, joining tables, running SQL queries, \n    and performing date-related operations using PySpark.\"\"\"\n    def __init__(self, enable_hive_support: bool = False, initial_spark_config: dict = None):\n        \"\"\"\n        Initializes the AzureTableOperations with optional Hive support and initial Spark configurations.",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "ConfigurationError",
        "kind": 6,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "class ConfigurationError(Exception):\n    pass\nclass AzureTableOperations:\n    \"\"\"Handles reading from and saving to abfss, reading and saving to catalog, joining tables, running SQL queries, \n    and performing date-related operations using PySpark.\"\"\"\n    def __init__(self, enable_hive_support: bool = False, initial_spark_config: dict = None):\n        \"\"\"\n        Initializes the AzureTableOperations with optional Hive support and initial Spark configurations.\n        The Spark session is initialized during this process.\n        \"\"\"",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "AzureTableOperations",
        "kind": 6,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "class AzureTableOperations:\n    \"\"\"Handles reading from and saving to abfss, reading and saving to catalog, joining tables, running SQL queries, \n    and performing date-related operations using PySpark.\"\"\"\n    def __init__(self, enable_hive_support: bool = False, initial_spark_config: dict = None):\n        \"\"\"\n        Initializes the AzureTableOperations with optional Hive support and initial Spark configurations.\n        The Spark session is initialized during this process.\n        \"\"\"\n        self.spark = None\n        self.enable_hive_support = enable_hive_support",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "tests.test_azure_table_operations",
        "description": "tests.test_azure_table_operations",
        "peekOfCode": "logger = logging.getLogger(__name__)\n# Custom Exceptions\nclass SparkInitializationError(Exception):\n    pass\nclass DataLoadError(Exception):\n    pass\nclass DataSaveError(Exception):\n    pass\nclass ConfigurationError(Exception):\n    pass",
        "detail": "tests.test_azure_table_operations",
        "documentation": {}
    },
    {
        "label": "TestAzureCredentials",
        "kind": 6,
        "importPath": "tests.test_credentials",
        "description": "tests.test_credentials",
        "peekOfCode": "class TestAzureCredentials(unittest.TestCase):\n    @patch('azure_credentials.ClientSecretCredential')\n    def test_credentials_initialization(self, MockCredential):\n        mock_credential = MockCredential.return_value\n        credentials = AzureCredentials('tenant_id', 'client_id', 'client_secret')\n        self.assertIsInstance(credentials.credential, ClientSecretCredential)\n        MockCredential.assert_called_once_with('tenant_id', 'client_id', 'client_secret')\n    @patch('azure_credentials.ClientSecretCredential')\n    def test_credentials_initialization_failure(self, MockCredential):\n        MockCredential.side_effect = Exception(\"Initialization error\")",
        "detail": "tests.test_credentials",
        "documentation": {}
    },
    {
        "label": "AzureBlobClient",
        "kind": 6,
        "importPath": "azure_blob_client",
        "description": "azure_blob_client",
        "peekOfCode": "class AzureBlobClient:\n    \"\"\"Handles operations with Azure Blob Storage.\"\"\"\n    def __init__(self, account_name: str, credential: ClientSecretCredential):\n        try:\n            self.blob_service_client = BlobServiceClient(\n                account_url=f\"https://{account_name}.blob.core.windows.net\",\n                credential=credential\n            )\n            logger.info(\"Azure Blob Service Client initialized successfully.\")\n        except Exception as e:",
        "detail": "azure_blob_client",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "azure_blob_client",
        "description": "azure_blob_client",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass AzureBlobClient:\n    \"\"\"Handles operations with Azure Blob Storage.\"\"\"\n    def __init__(self, account_name: str, credential: ClientSecretCredential):\n        try:\n            self.blob_service_client = BlobServiceClient(\n                account_url=f\"https://{account_name}.blob.core.windows.net\",\n                credential=credential\n            )\n            logger.info(\"Azure Blob Service Client initialized successfully.\")",
        "detail": "azure_blob_client",
        "documentation": {}
    },
    {
        "label": "DatabricksSparkConnector",
        "kind": 6,
        "importPath": "azure_table_operations",
        "description": "azure_table_operations",
        "peekOfCode": "class DatabricksSparkConnector:\n    def __init__(self, databricks_host: str = None, databricks_cluster_id: str = None,\n                 client_id: str = None, client_secret: str = None, tenant_id: str = None,\n                 storage_account_name: str = None, storage_account_key: str = None):\n        \"\"\"\n        Initializes the DatabricksConnector class with service principal credentials and storage account information.\n        Args:\n            databricks_host (str, optional): The Databricks workspace URL.\n            databricks_cluster_id (str, optional): The ID of the Databricks cluster to connect to.\n            client_id (str, optional): The client ID of the Azure AD service principal.",
        "detail": "azure_table_operations",
        "documentation": {}
    },
    {
        "label": "AzureCredentials",
        "kind": 6,
        "importPath": "credentials",
        "description": "credentials",
        "peekOfCode": "class AzureCredentials:\n    \"\"\"Handles Azure authentication and credential management.\"\"\"\n    def __init__(self, tenant_id: str, client_id: str, client_secret: str):\n        try:\n            self.credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n            logger.info(\"Azure credentials initialized successfully.\")\n        except Exception as e:\n            logger.error(f\"Error initializing Azure credentials: {e}\")\n            raise",
        "detail": "credentials",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "credentials",
        "description": "credentials",
        "peekOfCode": "logger = logging.getLogger(__name__)\nclass AzureCredentials:\n    \"\"\"Handles Azure authentication and credential management.\"\"\"\n    def __init__(self, tenant_id: str, client_id: str, client_secret: str):\n        try:\n            self.credential = ClientSecretCredential(tenant_id, client_id, client_secret)\n            logger.info(\"Azure credentials initialized successfully.\")\n        except Exception as e:\n            logger.error(f\"Error initializing Azure credentials: {e}\")\n            raise",
        "detail": "credentials",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def main():\n    \"\"\"Main function to run the Azure Data Loader.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Azure Data Loader\")\n    parser.add_argument(\"--tenant_id\", required=True, help=\"Azure Tenant ID\")\n    parser.add_argument(\"--client_id\", required=True, help=\"Azure Client ID\")\n    parser.add_argument(\"--client_secret\", required=True, help=\"Azure Client Secret\")\n    parser.add_argument(\"--account_name\", required=True, help=\"Azure Storage Account Name\")\n    parser.add_argument(\"--container_name\", required=True, help=\"Azure Data Lake Container Name\")\n    parser.add_argument(\"--file_path\", required=True, help=\"Path to the file in the Azure Data Lake\")\n    parser.add_argument(\"--chunksize\", type=int, default=1000000, help=\"Size of data chunks to process at a time\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef main():\n    \"\"\"Main function to run the Azure Data Loader.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Azure Data Loader\")\n    parser.add_argument(\"--tenant_id\", required=True, help=\"Azure Tenant ID\")\n    parser.add_argument(\"--client_id\", required=True, help=\"Azure Client ID\")\n    parser.add_argument(\"--client_secret\", required=True, help=\"Azure Client Secret\")\n    parser.add_argument(\"--account_name\", required=True, help=\"Azure Storage Account Name\")\n    parser.add_argument(\"--container_name\", required=True, help=\"Azure Data Lake Container Name\")\n    parser.add_argument(\"--file_path\", required=True, help=\"Path to the file in the Azure Data Lake\")",
        "detail": "main",
        "documentation": {}
    }
]