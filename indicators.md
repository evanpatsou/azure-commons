### 1. Environment Setup and Sample Data

Hereâ€™s the Spark code to set up `dataset1`, `dataset2`, and `dataset3`, and perform the merge as described.

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count
from pyspark.sql.types import StructType, StructField, IntegerType, StringType

# Initialize Spark Session
spark = SparkSession.builder \
    .appName("DatasetMergeExample") \
    .getOrCreate()

# Define schemas
schema1 = StructType([
    StructField("dataset1_key", IntegerType(), True),
    StructField("textcode", StringType(), True)
])

schema3 = StructType([
    StructField("dataset3_key", IntegerType(), True),
    StructField("textcode", StringType(), True)
])

# Sample data for dataset1
data1 = [
    (1, "textcode1"),
    (2, "textcode2"),
    (3, "textcode3"),
    (4, "textcode4"),
    (1, "textcode11"),
    (2, "textcode22")
]

# Sample data for dataset3
data3 = [
    (5, "textcode2"),
    (6, "textcode4"),
    (7, "textcode7"),
    (8, "textcode8")
]

# Create DataFrames
df_dataset1 = spark.createDataFrame(data1, schema1)
df_dataset3 = spark.createDataFrame(data3, schema3)

# Display dataset1 and dataset3
df_dataset1.show()
df_dataset3.show()
```

---

### 2. Join `dataset1` with `dataset3` on `textcode`

Perform an outer join on `textcode` to combine `dataset1` and `dataset3` based on overlapping `textcode` values.

```python
# Outer join on textcode to combine dataset1 and dataset3
df_combined = df_dataset1.join(df_dataset3, on="textcode", how="outer")

# Display combined dataset
df_combined.show()
```

### 3. Filter Rows to Keep the Most Complete Entries

We will:
1. **Identify complete rows** (those with both `dataset1_key` and `dataset3_key`).
2. **Prioritize these complete rows** for each `textcode`.
3. **Remove duplicate or redundant rows** where an entry with both keys exists for the same `textcode`.

```python
# Filter to keep the most complete rows, prioritizing rows with both dataset1_key and dataset3_key
from pyspark.sql import functions as F

df_most_complete = df_combined.withColumn(
    "row_priority", 
    when(col("dataset1_key").isNotNull() & col("dataset3_key").isNotNull(), 1)  # Row is complete if it has both keys
    .when(col("dataset1_key").isNotNull() | col("dataset3_key").isNotNull(), 2)  # Row has only one key, lower priority
)

# Use row_number to select the highest priority row for each textcode
from pyspark.sql.window import Window

window_spec = Window.partitionBy("textcode").orderBy("row_priority")
df_most_complete = df_most_complete.withColumn("row_num", F.row_number().over(window_spec)) \
    .filter(col("row_num") == 1) \
    .drop("row_priority", "row_num")

# Display the filtered dataset with only the most complete rows
df_most_complete.show()
```

The above code keeps only the most complete rows for each `textcode` value by assigning priorities and filtering based on those priorities.

---

### 4. Create the Final `dataset1_dataset3` by Selecting Relevant Columns

Now, we can select only the `dataset1_key` and `dataset3_key` columns to create the final merged dataset, `dataset1_dataset3`.

```python
# Select the desired columns to form the final dataset
df_dataset1_dataset3 = df_most_complete.select("dataset1_key", "dataset3_key").distinct()

# Display the final dataset1_dataset3
df_dataset1_dataset3.show()
```

---

### Expected Output for `dataset1_dataset3`

The final result should look like:

| dataset1_key | dataset3_key |
|--------------|--------------|
| 1            |              |
| 2            | 5            |
| 3            |              |
| 4            | 6            |
| 7            |              |
| 8            |              |

